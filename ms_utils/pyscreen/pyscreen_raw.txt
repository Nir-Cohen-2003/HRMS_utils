**************
1. config
**************
from dataclasses import dataclass, field
@dataclass
class search_config:
    polarity:str
    ms1_mass_tolerance:float=5e-6
    ms2_mass_tolerance:float=10e-6 # high, but it's because NIST can have pretty high mass error.
    DotProd_threshold:dict[int:int|float]=field(default_factory=lambda:{
                0:650,
                1:700,
                2:800,
                3:900})
    search_engine:str='entropy'
    noise_threshold:float=0.005

@dataclass
class isotopic_pattern_config:
    mass_tolerance : float
    ms1_resolution:float
    minimum_intensity : float=5e5
    max_intensity_ratio : float=1.7

@dataclass
class blank_config:
    ms1_mass_tolerance : float=3e-6
    dRT_min : float=0.1
    ratio : float | int=5
    use_ms2:bool=False
    dRT_min_with_ms2:float=0.3
    ms2_fit:float=0.85


@dataclass
class suspect_list_config:
    exclusion_list:str=None

@dataclass
class pyscreen_config:
    search:search_config=None
    isotopic_pattern:isotopic_pattern_config=None
    blank:blank_config=None
    suspect_list:suspect_list_config=None
    def __post_init__(self):
        if(isinstance(self.search,dict)):
            self.search = search_config(**self.search)
        if self.search is None:
            raise Exception('search configuration is not set! you need to give me at least polarity you know')
        
        if(isinstance(self.isotopic_pattern,dict)):
            self.isotopic_pattern = isotopic_pattern_config(**self.isotopic_pattern)
        if self.isotopic_pattern is None:
            pass #TODO: hnadle this better or add a check laer for nullity of isotopic pattern
        
        if(isinstance(self.blank,dict)):
            self.blank = blank_config(**self.blank)
        if self.blank is None:
            self.blank = blank_config()
        
        if(isinstance(self.suspect_list,dict)):
            self.suspect_list = suspect_list_config(**self.suspect_list)
        if self.suspect_list is None:
            self.suspect_list = suspect_list_config()

if __name__ == "__main__":

    # print(blank_config(**{
    #         'ms1_mass_tolerance':3e-6,
    #         'dRT_min':0.1,
    #         'ratio':5, 
    #         'use_ms2':False,
    #         'dRT_min_with_ms2':0.5, 
    #         'ms2_fit':0.85
    #     }))

    config = {
        'search':
        {
            'polarity':'positive',
        },
        'isotopic_pattern':
        {
            'mass_tolerance' : 3*1e-6, 
            'max_intensity_ratio' : 1.7,
            'minimum_intensity' : 1e5,
            'ms1_resolution':0.7e5,
        },
    }
    config = pyscreen_config(**config)
    print(config)

**************
2. EPA_utils
**************
import polars as pl
from pyscreen_constants import EPA_important_columns
from pathlib import Path
from config import suspect_list_config
def exclude_boring_compounds(EPA:pl.DataFrame) -> pl.DataFrame:
    '''
    NON VALIDATED! DO NOT USE! this is Nir's quick-n-dirty exclusion list
    overwrites the Haz_level of the natural compounds to be 3, mathces based on inchikey and DTXSID
    '''
    print("NON VALIDATED! DO NOT USE! this is Nir's quick-n-dirty exclusion list")
    boring_compounds = pl.read_excel(source=Path(r"boring_compounds.xlsx"))
    boring_compounds = boring_compounds.select(['DTXSID','inchikey']).unique()
    boring_compounds = boring_compounds.rename({
        'inchikey' :'inchikey_EPA'
    },strict=False)
    EPA_boring = EPA.join(boring_compounds,on=['DTXSID','inchikey_EPA'],how='inner')
    EPA_boring = EPA_boring.with_columns(
        pl.lit(value=3,dtype=pl.Int64).alias('Haz_level')
    )
    EPA = EPA.join(boring_compounds,on=['DTXSID','inchikey_EPA'],how='anti')
    EPA = pl.concat([EPA,EPA_boring])
    return EPA

def get_EPA(config:suspect_list_config)-> pl.DataFrame:
    EPA = pl.scan_parquet(r"EPA_with_Haz_level.parquet").select(EPA_important_columns).rename(
            {'INCHIKEY':'inchikey_EPA',
             'CASRN':'CAS_EPA',
             'synonyms':'Synonyms_EPA',
             'PREFERRED NAME':'Name_EPA',
             'MOLECULAR FORMULA':'Formula_EPA',
             },
        strict=False
    ).collect()

    if config.exclusion_list is None:
        pass
    else:
        match config.exclusion_list.lower():
            case 'boring_compounds':
                EPA = exclude_boring_compounds(EPA)
            case None:
                pass
            case 'none':
                pass
            case _:
                print(config.exclusion_list)
                raise Exception("invalid exclusion list given")
    return EPA
**************
3. GUI_script
**************


import tkinter as tk
from tkinter import ttk
from tkinter import filedialog
import main
import os
from pathlib import Path
from time import time

sample_file_paths = None
blank_file_path = None
mode = None
    
def choose_spectra_file():
    global sample_file_paths
    filetypes = [('All files', '*.txt')]
    file_list = filedialog.askopenfilenames(filetypes=filetypes, initialdir='')
    sample_file_paths = [Path(file) for file in file_list]
    # return sample_file_paths

def choose_blank_file():
    global blank_file_path
    filetypes = [('All files', '*.txt')]
    file = filedialog.askopenfilename(filetypes=filetypes, initialdir='')
    blank_file_path = Path(file)
    # return blank_file_path
    
    
def click():
    global mode
    mode =  operation_mode_choice.get()

    
def run_script():
    
    mode =  operation_mode_choice.get()
    search_mode = search_mode_choice.get()

    if search_mode =='Use previous results':
        search_mode = False
    elif search_mode =="Don't use previous results":
        search_mode = True
    else:
        raise Exception("search mode Error")


    if not isinstance(sample_file_paths,list):
        print("ERROR: Must choose sample files, at least one")
        raise Exception("sample file Error")

    if not (isinstance(sample_file_paths[0],Path) or isinstance(sample_file_paths[0])):
        print("ERROR: Must choose sample files, at least one")
        raise Exception("sample file Error2")
    
    run_button.destroy()

    progress_window = tk.Toplevel(root)
    progress_window.title("run in progress")
    progress_window.geometry('600x300')
    tk.Label(progress_window,text="run in progress").pack()
    print(sample_file_paths)
    print(blank_file_path)
    print(mode)
    start = time()
    main.main(sample_file_paths=sample_file_paths,blank_file_path=blank_file_path,mode=mode,ignore_previous_results=search_mode)
    print("this took: " + str(time()-start)+" seconds")
    root.destroy()

    # root.after(5, main.main,arg1=sample_file_paths,arg2=blank_file_path,arg3=mode)

    # new_window = tk.Tk()
    # new_window.title("results analysis- in progress")
    # new_window.geometry('600x300')
    # lbl1 = tk.Label(new_window, text="WORKING, don't close this window")
    # lbl1.grid(column=0, row=0)
    
    # new_window.mainloop()
    # new_window.quit()
    # new_window.destroy()
    
if __name__ == "__main__":
    
    #input_file_list = []
    root = tk.Tk()
    root.title("Orbitrap results analysis")
    
    root.geometry('600x300')
    
    files_label = tk.Label(root, text="enter the files you want to run: ")
    files_label.grid(column=0, row=0)
    files_button = tk.Button(root, text = "files", fg='black', command=choose_spectra_file)
    files_button.grid(column=1, row=0)
    
    
    blank_label = tk.Label(root, text="enter the blank file: ")
    blank_label.grid(column=0, row=4)
    blank_button = tk.Button(root, text = "blank file", fg='black', command=choose_blank_file)
    blank_button.grid(column=1, row=4)
    
    mode_label = tk.Label(root, text="choose operation mode: ")
    mode_label.grid(column=0, row=8)
    operation_mode_choice = ttk.Combobox(root, values = ['positive', 'negative'])
    operation_mode_choice.grid(column=1, row=8)

    search_label = tk.Label(root, text="choose search mode: ")
    search_label.grid(column=0, row=12)
    search_mode_choice = ttk.Combobox(root, values = ['Use previous results', "Don't use previous results"])
    search_mode_choice.grid(column=1, row=12)

    # mode_button = tk.Button(root, text = "set mode", fg='black', command=click)
    # mode_button.grid(column=2, row=8)
    
    run_button = tk.Button(root, text = "run program", fg='black', command=run_script)#lambda: check(input_path_formatted, input_file_list, input_charge, input_blank))
    run_button.grid(column=1, row=16)
    
    root.mainloop()
**************
4. isotope_pattern
**************
import numpy as np
import re
from pyscreen_constants import NITROGEN_SEPARATION_RESOLUTION
from config import isotopic_pattern_config

isotopic_pattern_arr = np.array( 
    #mass difference, zero isiotope probability, first isoptope probability
    [
        [1.0034,0.989,0.011], #C
        [0.99703,0.996,0.004], #N
        [1.9958,0.95,0.042], #S
        [1.9971,0.758,0.242], #Cl
        [1.998,0.507,0.493], #Br
    ]
)

def fits_isotopic_pattern(mzs,intensities,formula:str,precursor_mz:float,config:isotopic_pattern_config):
    mzs = np.array(mzs)
    intensities = np.array(intensities)
    element_numbers = get_element_numbers(formula=formula)
    element_fits = np.full_like(element_numbers,fill_value=False,dtype=np.bool)
    
    precursor_index = (np.abs(mzs-precursor_mz)).argmin()
    precursor_intensity = intensities[precursor_index]

    #check for C and N
    if config.ms1_resolution > NITROGEN_SEPARATION_RESOLUTION:
        element_fits[0] = check_element_fit(config,0,mzs,intensities,element_numbers[0],precursor_mz,precursor_intensity) #C
        element_fits[1] = check_element_fit(config,1,mzs,intensities,element_numbers[1],precursor_mz,precursor_intensity) #N
    else:
        CN_fit = check_CN_fit(config,mzs,intensities,element_numbers[0],element_numbers[1],precursor_mz,precursor_intensity)
        element_fits[0] = CN_fit
        element_fits[1] = CN_fit

    # check for S,Cl,Br
    for i in [2,3,4]:
        element_fits[i] = check_element_fit(config,i,mzs,intensities,element_numbers[i],precursor_mz,precursor_intensity)
    
    return np.all(element_fits)

fits_isotopic_pattern_batch =np.vectorize(fits_isotopic_pattern)

def get_element_numbers(formula:str) -> np.ndarray:
    has_C = re.search(r"C((\d+)|[A-Z]|$)",formula)
    if has_C is None:
        num_C = 0
    else:
        num_C = re.search(r"C(\d+)",has_C.group(0))
        if num_C is None:
            num_C = 1
        else:
            num_C = int(num_C.group(1))
    has_N = re.search(r"N((\d+)|[A-Z]|$)",formula)
    if has_N is None:
        num_N = 0
    else:
        num_N = re.search(r"N(\d+)",has_N.group(0))
        if num_N is None:
            num_N = 1
        else:
            num_N = int(num_N.group(1))
    has_S = re.search(r"S((\d+)|[A-Z]|$)",formula)
    if has_S is None:
        num_S = 0
    else:
        num_S = re.search(r"S(\d+)",has_S.group(0))
        if num_S is None:
            num_S = 1
        else:
            num_S = int(num_S.group(1))
    has_Cl = re.search(r"Cl((\d+)|[A-Z]|$)",formula)
    if has_Cl is None:
        num_Cl = 0
    else:
        num_Cl = re.search(r"Cl(\d+)",has_Cl.group(0))
        if num_Cl is None:
            num_Cl = 1
        else:
            num_Cl = int(num_Cl.group(1))
    has_Br = re.search(r"Br((\d+)|[A-Z]|$)",formula)
    if has_Br is None:
        num_Br = 0
    else:
        num_Br = re.search(r"Br(\d+)",has_Br.group(0))
        if num_Br is None:
            num_Br = 1
        else:
            num_Br = int(num_Br.group(1))

    return np.array([num_C,num_N,num_S,num_Cl,num_Br])

def check_element_fit(
        config:isotopic_pattern_config,
        i:int,
        mzs:np.ndarray,
        intensities:np.ndarray,
        element_number:int,
        precursor_mz:float,
        precursor_intensity:float):
    if element_number == 0:
        return True
    else:
        zero_isotope_intesnsity = np.power(isotopic_pattern_arr[i][1],element_number)
        computed_isotope_relative_intensity = isotopic_pattern_arr[i][2]*element_number/zero_isotope_intesnsity
        computed_isotope_intensity = precursor_intensity*computed_isotope_relative_intensity
        if computed_isotope_intensity < config.minimum_intensity: #meaning the intensity would be too small, and we won't be able to detect it reliably anyway.
            return True
        computed_isotope_mass = precursor_mz + isotopic_pattern_arr[i][0]
        best_fit_index = (np.abs(mzs-computed_isotope_mass)).argmin()
        isotope_mass = mzs[best_fit_index]
        
        if not np.isclose(isotope_mass,computed_isotope_mass,rtol=config.mass_tolerance): # then the element probably isn't even present, or the intensity is too low.
            return False
        else:  
            isotope_intensity = intensities[best_fit_index]
            intensity_ratio = isotope_intensity/computed_isotope_intensity
            if intensity_ratio > config.max_intensity_ratio or intensity_ratio < 1/config.max_intensity_ratio: # so we have the right isotope mass, but the ratio is off, meaning we don't have the correct number
                return False
            else:
                return True

def check_CN_fit(
        config:isotopic_pattern_config,
        mzs:np.ndarray,
        intensities:np.ndarray,
        C_number:int,
        N_number:int,
        precursor_mz:float,
        precursor_intensity:float):
    '''checks for fit of the combined N and C isotopic ratio'''
    if N_number == 0:
        return check_element_fit(config,0,mzs,intensities,C_number,precursor_mz,precursor_intensity)
    if C_number == 0:
        return check_element_fit(config,1,mzs,intensities,N_number,precursor_mz,precursor_intensity)
    
    zero_isotope_intesnsity = np.power(isotopic_pattern_arr[0][1],C_number)*np.power(isotopic_pattern_arr[1][1],N_number)
    computed_isotope_relative_intensity = (isotopic_pattern_arr[0][2]*C_number+isotopic_pattern_arr[1][2]*N_number)/zero_isotope_intesnsity
    computed_isotope_intensity = precursor_intensity*computed_isotope_relative_intensity
    if computed_isotope_intensity < config.minimum_intensity: #meaning the intensity would be too small, and we won't be able to detect it reliably anyway.
        return True
    computed_isotope_mass = precursor_mz + isotopic_pattern_arr[0][0] # the mass of C will be the main mass, since the N peak is smaller and will be merged to it.
    best_fit_index = (np.abs(mzs-computed_isotope_mass)).argmin()
    isotope_mass = mzs[best_fit_index]
    
    if not np.isclose(isotope_mass,computed_isotope_mass,rtol=config.mass_tolerance): # then the element probably isn't even present, or the intensity is too low.
        return False
    else:  
        isotope_intensity = intensities[best_fit_index]
        intensity_ratio = isotope_intensity/computed_isotope_intensity
        if intensity_ratio > config.max_intensity_ratio or intensity_ratio < 1/config.max_intensity_ratio: # so we have the right isotope mass, but the ratio is off, meaning we don't have the correct number
            return False
        else:
            return True



if __name__ == "__main__":
    mzs = []#[246.12357, 247.12074, 247.12688, 248.12413, 248.13025]	
    intensities =[]#[230812832.0, 2362436.0, 30514068.0, 473412.0, 2097496.0]
    formula = "C13H15N3O2"
    precursor_mz=246.124
    config = isotopic_pattern_config(mass_tolerance=3e-6,ms1_resolution=1.2e5,minimum_intensity=5e5,max_intensity_ratio=1.7)
    fit=fits_isotopic_pattern(mzs=mzs,intensities=intensities,precursor_mz=precursor_mz,formula=formula,config=config)
    print(fit)
    print(fit.shape)
    print(type(fit))

**************
5. main
**************
import polars as pl
from pathlib import Path
from time import time
from NIST_search import get_NIST
from EPA_utils import get_EPA
from read_MSDIAL import get_chromatogram
from NIST_MSDIAL import  *
from config import *
VERBOSE = True
SHORT = False

def screen_per_file(
        MSDIAL_file_path: str | Path, 
        blank_chromatogram: pl.DataFrame,
        NIST:pl.DataFrame,
        EPA: pl.DataFrame,
        config:pyscreen_config
        ):
    '''
    Takes an MSDIAL export file, reads and cleans it.
    Then crosses it with EPA, and filters based on Height and Haz_level
    then sends all suspects (each peak only once, since this is a time intensive operation) to NIST.
    if previous results are present, uses them.
    reads the reuslts, filters based on molecular identifier and iddentification quality (again- based on haz_level)
    writes the results to excel.'''
    print(f'MSDIAL_file_path ' + str(MSDIAL_file_path))

    chromatogram = get_chromatogram(MSDIAL_file_path)
    start_blank_subtraction = time()
    if blank_chromatogram is not None: #meaning there is a blank file
        chromatogram = subtract_blank_frame(sample_df=chromatogram,blank_df=blank_chromatogram,config=config.blank)
    end_blank_subtraction = time()
    if VERBOSE:
        print('blank subtraction done')
        print(chromatogram.shape)
        print(chromatogram.schema)
        print(f'time to subtract: {end_blank_subtraction-start_blank_subtraction}')
        chromatogram.write_excel(MSDIAL_file_path.parent.joinpath(MSDIAL_file_path.stem+"_chromatogram.xlsx"))
    if SHORT:
        chromatogram = chromatogram.head(1000)

    start_cross = time()
    suspects = cross_with_EPA(chromatogram=chromatogram,EPA=EPA,config=config.search)
    
    suspects = annotate_isotopic_pattern(suspects=suspects,config=config.isotopic_pattern)
    suspects = filter_suspects(suspects=suspects,chromatogram=chromatogram)
    end_cross = time()
    if VERBOSE:
        print('got suspects')
        print(suspects.shape)
        print(suspects.columns)
        print(f'time to cross: {end_cross-start_cross}')
        suspects.write_excel(MSDIAL_file_path.parent.joinpath(MSDIAL_file_path.stem+"_suspects.xlsx"))

    start_search = time()

    NIST_search_results = search_in_NIST(
        suspects=suspects, 
        NIST=NIST,
        chromatogram=chromatogram,
        MSDIAL_file_path=MSDIAL_file_path,
        config=config.search)
    end_search = time()
    if VERBOSE:
        print('searched against NIST')
        print(NIST_search_results.shape)
        print(NIST_search_results.columns)
        print(f'time to search NIST: {end_search-start_search}')
        NIST_search_results.write_excel(MSDIAL_file_path.parent.joinpath(MSDIAL_file_path.stem+"_NIST_search_results.xlsx"))

    start_result_crossing = time()
    suspects_found_in_NIST = cross_NIST_results_with_suspects(
        NIST_search_results=NIST_search_results,
        suspects=suspects,
        config=config.search)
    end_result_crossing = time()
    if VERBOSE:
        print('crossed NIST results with suspects')
        print(suspects_found_in_NIST.shape)
        print(suspects_found_in_NIST.columns)
        print(f'time to cross results: {end_result_crossing-start_result_crossing}')
        suspects_found_in_NIST.write_excel(MSDIAL_file_path.parent.joinpath(MSDIAL_file_path.stem+"_suspects_found_in_NIST.xlsx"))

    suspects_found_in_NIST = foramt_results(
        suspects_found_in_NIST=suspects_found_in_NIST,
        NIST=NIST,
        EPA=EPA,
        chromatogram=chromatogram)

    start_writing_results= time()
    suspects_found_in_NIST.sort(
            by=['Haz_level','DotProd'],descending=[False,True]
        ).write_excel(MSDIAL_file_path.parent.joinpath(MSDIAL_file_path.stem+"_results.xlsx"))
    suspects_found_in_NIST = None # memory savings
    end_writing_results = time()
    if VERBOSE:
        print(f'time to write results: {end_writing_results-start_writing_results}')
    
    start_not_in_lib = time()
    suspects_not_in_lib = not_in_lib(
        NIST_search_results=NIST_search_results,
        suspects=suspects,
        NIST=NIST,
        EPA=EPA,
        config=config.search)
    if VERBOSE:
        print('not in lib')
        print(suspects_not_in_lib.shape)
        print(suspects_not_in_lib.columns)
    end_not_in_lib = time()
    if VERBOSE:
        print(f'time to do not in lib: {end_not_in_lib-start_not_in_lib}')

    suspects_not_in_lib.write_excel(MSDIAL_file_path.parent.joinpath(MSDIAL_file_path.stem+"_not_in_lib.xlsx"))
    if VERBOSE:
        print('wrote not in lib')
    return

# Takes a list of files (as paths), a blank (as path) a polarity (positive/negative).
# Then splits the work using the "screen_per_file" function.
# currently impossible to make parrallel, since NIST cant work this way and its the most time consuming step by a mile.
def main(
        sample_file_paths:  list[str] | list[Path] , 
        blank_file_path : str | Path | None, 
        config:pyscreen_config | dict
        ):
    
    if isinstance(config,dict):
        config = pyscreen_config(**config) ## converts it to the correct format and sets default values. also works when reading from yaml

    if blank_file_path in sample_file_paths:
        sample_file_paths.remove(blank_file_path)
    # tells you what it got.
    print('sample files:')
    for path in sample_file_paths:
        print(str(path))
    print('blank file')
    print(str(blank_file_path))
    print('config:')
    print(config)
    

    NIST = get_NIST(condig=config.search)
    EPA = get_EPA(config=config.suspect_list)

    if VERBOSE:
        print('fetched DBs')
    # some error checking
    

    if len(sample_file_paths) <1 :
        raise Exception("Error: must have at least one sample file")


    if blank_file_path is not None:
        blank_chromatogram = get_chromatogram(path=blank_file_path)
    else:
        blank_chromatogram = None

    for MSDIAL_file_path in sample_file_paths:
        try:
            screen_per_file(MSDIAL_file_path=MSDIAL_file_path,blank_chromatogram=blank_chromatogram,NIST=NIST,EPA=EPA,config=config)
        except Exception as e:
            print("error in file: "+ str(MSDIAL_file_path))
            print(e)
            raise e


if __name__ == "__main__":
    start = time()
    sample_dir = Path(r"")
    sample_file_paths = list(sample_dir.glob(pattern=r'*.txt',case_sensitive=True))
    sample_file_paths = [
        Path(r""),
        
    ]

    blank_file_path = Path(r"")
    config = {
        'search':
        {
            'polarity':'positive',
        },
        'isotopic_pattern':
        {
            'mass_tolerance' : 3*1e-6, 
            'ms1_resolution':1.2e5,
        },
    }

    main(
        sample_file_paths=sample_file_paths,
        blank_file_path=blank_file_path,
        config=config)

    print("time: "+str(time()-start))
**************
6. NIST_MSDIAL
**************
import polars as pl
import numpy as np
from pathlib import Path
from NIST_search import NIST_search_external , custom_search
from pyscreen_constants import adducts_neg, adducts_pos
from spectral_similarity import entropy_score_batch
from isotope_pattern import fits_isotopic_pattern_batch
from config import *
VERBOSE = False
SHORT = False

def subtract_blank_frame(
        sample_df: pl.DataFrame, 
        blank_df: pl.DataFrame, 
        config:blank_config) -> pl.DataFrame:
    '''subtracts a blank chromatogram, using ms1, ms2 and RT. 
    in absense of ms2 for either the blank or the sample compound, a stricter rt threshold is used. 
    keep dRT_min_with_ms2 > dRT_min, or the logic gets weird and wrong.'''

    if not config.use_ms2: #so when both sample and blank spectra has msms, we require a 0.85 fit on ms2, but lower fit on rt. if any of them lacks ms2, we just use strict rt.
        sample_lf = sample_df.select(
            [
                "Peak ID",
                "RT (min)",
                "Precursor_mz_MSDIAL",
                "Height",
            ]
        ).lazy()
        blank_lf = blank_df.select(
            [
                "RT (min)",
                "Precursor_mz_MSDIAL",
                "Height",
            ]
        ).lazy()

        subtract_df = sample_lf.join_where(
            blank_lf,
            pl.col("RT (min)") < pl.col("RT (min)_blank") + config.dRT_min,
            pl.col("RT (min)") > pl.col("RT (min)_blank") -  config.dRT_min,
            (pl.col("Precursor_mz_MSDIAL").truediv(pl.col("Precursor_mz_MSDIAL_blank"))-1).abs().le(config.ms1_mass_tolerance),
            pl.col("Height") <  pl.col("Height_blank") * config.ratio,
            suffix="_blank"
        ).collect(streaming=True)
    else: # so we just use strict rt
        sample_lf = sample_df.select(
            [
                "Peak ID",
                "RT (min)",
                "Precursor_mz_MSDIAL",
                "Height",
                'msms_m/z',
                'msms_intensity'
            ]
        ).lazy()
        blank_lf = blank_df.select(
            [
                "RT (min)",
                "Precursor_mz_MSDIAL",
                "Height",
                'msms_m/z',
                'msms_intensity'
            ]
        ).lazy()
        subtract_lf = sample_lf.join_where(
            blank_lf,
            pl.col("RT (min)") < pl.col("RT (min)_blank") + config.dRT_min_with_ms2,
            pl.col("RT (min)") > pl.col("RT (min)_blank") - config.dRT_min_with_ms2,
            (pl.col("Precursor_mz_MSDIAL").truediv(pl.col("Precursor_mz_MSDIAL_blank"))-1).abs().le(config.ms1_mass_tolerance),
            pl.col("Height") <  pl.col("Height_blank") * config.ratio,
            suffix="_blank"
        )
        subtract_lf_rt_strict = subtract_lf.filter(
            pl.col('msms_m/z').is_null() | 
            pl.col('msms_m/z_blank').is_null()
        )
        subtract_lf_rt_strict = subtract_lf_rt_strict.filter(
            pl.col("RT (min)") < pl.col("RT (min)_blank") + config.dRT_min,
            pl.col("RT (min)") > pl.col("RT (min)_blank") - config.dRT_min
        )

        subtract_df_ms2 = subtract_lf.filter(
            pl.col('msms_m/z').is_not_null(),
            pl.col('msms_m/z_blank').is_not_null()
        ).collect(streaming=True)
        
        subtract_df_ms2 = subtract_df_ms2.filter(
            pl.struct(
                pl.col('msms_intensity'),
                pl.col('msms_m/z'),
                pl.col('msms_intensity_blank'),
                pl.col('msms_m/z_blank')
            ).map_batches(
                lambda spectra: entropy_score_batch(
                    spectra.struct.field('msms_m/z').to_numpy(),
                    spectra.struct.field('msms_intensity').to_numpy(),
                    spectra.struct.field('msms_m/z_blank').to_numpy(),
                    spectra.struct.field('msms_intensity_blank').to_numpy(),
                    config
                    ),
                return_dtype=pl.Float64,
                is_elementwise=True
            ).ge(config.ms2_fit))
        
        subtract_df = pl.concat([subtract_df_ms2,subtract_lf_rt_strict.collect(streaming=True)])

    cleaned_sample_df = sample_df.join(subtract_df,on="Peak ID", how='anti')
    return cleaned_sample_df


def cross_with_EPA(chromatogram : pl.LazyFrame | pl.DataFrame, EPA: pl.LazyFrame | pl.DataFrame, config:search_config) -> pl.DataFrame:
    '''
    crosses with EPA based on exact mass and isotopic pattern, with tolerance defined in the global constants file. 
    then fitlers based on Height and haz level.
    every Peak ID can get more than one suspect.'''
    chromatogram_lf = chromatogram.select(
        [
            'Peak ID',
            'Precursor_mz_MSDIAL',
            'Precursor_type_MSDIAL',
            'Height',
            'ms1_isotopes_m/z', 'ms1_isotopes_intensity'

        ]
    ).lazy()
    EPA_lf=EPA.select(
        [
            'DTXSID',
            'MONOISOTOPIC MASS',
            'Haz_level',
            'inchikey_EPA',
            'Formula_EPA'
        ]
    ).lazy()

    match  config.polarity.lower():
        case"positive": 
            adducts = adducts_pos
        case "negative":
            adducts = adducts_neg
        case _:
            print("ERROR: Must choose operational mode, positive or negative")
            raise Exception("Mode Error: mode was not set, or is not positive/negative!")
        
    adducts_list = []
    for adduct in adducts.keys():
        adduct_mass = adducts[adduct]
        suspects = chromatogram_lf.join_where(
            EPA_lf,
            pl.col('Precursor_mz_MSDIAL') - adduct_mass < pl.col('MONOISOTOPIC MASS').mul(1+config.ms1_mass_tolerance),
            pl.col('Precursor_mz_MSDIAL') - adduct_mass >  pl.col('MONOISOTOPIC MASS').mul(1-config.ms1_mass_tolerance),
            pl.col('Precursor_type_MSDIAL').str.contains(adduct,literal=True)
        )
        adducts_list.append(suspects)
    
    if VERBOSE:
        print('all adduct done')
    
    suspects = pl.concat(adducts_list,how='vertical')
    adducts_list = None #memory savings

    suspects=suspects.collect(streaming=True)
    return suspects

def annotate_isotopic_pattern(suspects:pl.DataFrame,config:isotopic_pattern_config) -> pl.DataFrame:
    suspects = suspects.with_columns(
        pl.struct(
            pl.col('ms1_isotopes_m/z'),
            pl.col('ms1_isotopes_intensity'),
            pl.col('Formula_EPA'),
            pl.col('Precursor_mz_MSDIAL')
        ).map_batches(
            function= lambda x: fits_isotopic_pattern_batch(
                x.struct.field('ms1_isotopes_m/z'),
                x.struct.field('ms1_isotopes_intensity'),
                x.struct.field('Formula_EPA'),
                x.struct.field('Precursor_mz_MSDIAL'),
                config
            ), is_elementwise=True, return_dtype=pl.Boolean
        ).alias('isotopic_pattern_match')
    )

    suspects = suspects.drop(
        ['ms1_isotopes_m/z', 'ms1_isotopes_intensity']
    )
    return suspects

def filter_suspects(suspects:pl.DataFrame,chromatogram:pl.DataFrame)->pl.DataFrame:
    Height_threshold = chromatogram.select(pl.col('Height')).min().item(0,0)  # minimal height, represents the limit of sensitivity
    suspects = suspects.filter(
        pl.col('Haz_level').eq(0)  |
        pl.col('Haz_level').eq(1) & pl.col('Height').gt(2.5*Height_threshold) |
        pl.col('Haz_level').eq(2) & pl.col('Height').gt(25*Height_threshold) |
        pl.col('Haz_level').eq(3) & pl.col('Height').gt(250*Height_threshold)
    )
    return suspects


def search_in_NIST(
    suspects: pl.DataFrame,
    NIST: pl.DataFrame,
    chromatogram:pl.DataFrame,
    MSDIAL_file_path : Path | str,
    config:search_config
    ) -> pl.DataFrame:
    
    suspect_to_NIST = chromatogram.join(
        suspects, on='Peak ID', how='semi'
    ).filter(
        pl.col('msms_m/z').is_not_null()
    ).select(
        [
            'Peak ID',
            'Precursor_mz_MSDIAL',
            'msms_intensity',
            'msms_m/z',
        ], 
    )
   
    #### TESTING ONLY #####
    if SHORT:
        suspect_to_NIST = suspect_to_NIST.head(100) 
    ######################

    if config.search_engine.lower() == 'nist': # so through the MS search software, which is slow but gives a graphical interface
        if isinstance(suspect_to_NIST,pl.LazyFrame):
            suspect_to_NIST = suspect_to_NIST.collect(streaming=True)
        NIST_search_results = NIST_search_external(
            query_df=suspect_to_NIST,
            NIST=NIST,
            MSDIAL_file_path=MSDIAL_file_path,
            ignore_previous_results=True)   
    else:
        NIST_search_results = custom_search(query_df=suspect_to_NIST,NIST=NIST,config=config)

    NIST_search_results = NIST_search_results.join(NIST.drop('Synonyms_NIST',strict=False),on='NIST_ID')
    NIST_search_results = NIST_search_results.select(
        [
            'NIST_ID', 
            'Peak ID', 
            'DotProd', 
            'Name_NIST', 
            'DB_ID', 
            'DB_Name', 
            'PrecursorMZ',
            'Precursor_type_NIST', 
            'Instrument_type', 
            'Formula_NIST', 
            'Num_Peaks', 
            'inchikey_NIST',
            'collision_energy_NIST' 
            ]
    )
    return NIST_search_results


def cross_NIST_results_with_suspects(
    NIST_search_results : pl.DataFrame,
    suspects: pl.DataFrame,
    config:search_config
    ):
    suspects_lf = suspects.lazy()
    NIST_search_results_lf = NIST_search_results.lazy()

    # checks if the hit mathces the suspect.
    # pl.Config.set_streaming_chunk_size(size=100) # this lowers maximal memory usage, in principal. I'm really not sure it works.

    suspects_found_in_NIST = suspects_lf.join(
        NIST_search_results_lf,
        left_on=['Peak ID','Precursor_type_MSDIAL','inchikey_EPA'],
        right_on=['Peak ID',"Precursor_type_NIST",'inchikey_NIST'],
        how='inner',
        coalesce=False
    ).drop(
        ['Precursor_type_NIST','inchikey_NIST','Peak ID_right']
        ).rename(
        {
            'Precursor_type_MSDIAL': 'Precursor_type',
            'inchikey_EPA': 'inchikey'
         }
    ).collect()
    #TODO: think what do to when suspects returns with several identifications, some are wiht the same inchikey as suspected some are different.

    if VERBOSE:
        print('joined NIST search results with suspect list')
        print(suspects_found_in_NIST.shape)
        print(suspects_found_in_NIST.columns)
    suspects_found_in_NIST = suspects_found_in_NIST.filter( # filters based on quality of detection, lower threshold for lower Haz index (i.e. "scarier")
        pl.col('DotProd').ge(config.DotProd_threshold[0]) & pl.col('Haz_level').eq(0) |
        pl.col('DotProd').ge(config.DotProd_threshold[1]) & pl.col('Haz_level').eq(1) |
        pl.col('DotProd').ge(config.DotProd_threshold[2]) & pl.col('Haz_level').eq(2) |
        pl.col('DotProd').ge(config.DotProd_threshold[3]) & pl.col('Haz_level').eq(3)
        )
    if VERBOSE:
        print('filtered based on dotprod and Haz level')
        print(suspects_found_in_NIST.shape)
        print(suspects_found_in_NIST.columns)
    
    # suspects_found_in_NIST = suspects_found_in_NIST.lazy()
    suspects_found_in_NIST = suspects_found_in_NIST.sort('DotProd')
    if VERBOSE:
        print('sorted based on dotprod')
        print(suspects_found_in_NIST.shape)
        print(suspects_found_in_NIST.columns)
    # this removes the fit to several energies, since we already made sure nist found the same compound as we suspected it is
    suspects_found_in_NIST = suspects_found_in_NIST.group_by(['Peak ID','DTXSID'],maintain_order=True).last() #TODO:check if using unique is better
    # suspects_found_in_NIST = suspects_found_in_NIST.collect(streaming=True)
    if VERBOSE:
        print('grouped')
 
    return suspects_found_in_NIST

# returns the suspects with haz level 1 or 2 which without good identification (750 DotProd),
# which are also not present in NIST.
# TODO: fix this to work with my search
def not_in_lib(
        NIST_search_results:pl.DataFrame, 
        suspects:pl.DataFrame,
        NIST:pl.DataFrame,
        EPA:pl.DataFrame,
        config:search_config) -> pl.DataFrame:
    # must be lazy, otherwise fails
    # NIST_search_results = NIST_search_results.lazy()
    # suspects = suspects.lazy()
    # all_compounds_in_NIST = NIST.select(['inchikey_NIST','PrecursorMZ']).lazy()
    all_compounds_in_NIST = NIST.select(['inchikey_NIST','PrecursorMZ']).lazy()
    results_with_good_id = NIST_search_results.filter(pl.col('DotProd').ge(750)).lazy()
    suspects_lf = suspects.lazy()
    EPA_names = EPA.select(['DTXSID','Name_EPA']).lazy()

    suspects_found = suspects_lf.join(
        results_with_good_id, 
        left_on='inchikey_EPA',
        right_on='inchikey_NIST'
        ).filter(
        pl.col('Precursor_mz_MSDIAL').ge(pl.col('PrecursorMZ').mul(1-config.ms1_mass_tolerance)),
        pl.col('Precursor_mz_MSDIAL').le(pl.col('PrecursorMZ').mul(1+config.ms1_mass_tolerance)),
        )
    
    suspects_not_discovered_in_sample=suspects_lf.join(suspects_found,on='Peak ID',how='anti')

    suspects_not_present_in_sample = suspects_not_discovered_in_sample.join( 
        # this finds the suspects that do exist in NIST, but where not found in search, and hence are not present in the given sample
        all_compounds_in_NIST, 
        left_on='inchikey_EPA',
        right_on='inchikey_NIST'
        ).filter(
        pl.col('Precursor_mz_MSDIAL').ge(pl.col('PrecursorMZ').mul(1-config.ms1_mass_tolerance)),
        pl.col('Precursor_mz_MSDIAL').le(pl.col('PrecursorMZ').mul(1+config.ms1_mass_tolerance)),
       )
    
    suspects_not_in_lib = suspects_not_discovered_in_sample.join(suspects_not_present_in_sample,on='Peak ID',how='anti')
    important_suspects_not_in_lib = suspects_not_in_lib.filter(pl.col('Haz_level').le(1))
    important_suspects_not_in_lib = important_suspects_not_in_lib.join(EPA_names,on='DTXSID',how='left')
    return important_suspects_not_in_lib.collect(streaming=True)


def foramt_results(
        suspects_found_in_NIST:pl.DataFrame,
        NIST:pl.DataFrame,
        EPA:pl.DataFrame,
        chromatogram:pl.DataFrame
        ) -> pl.DataFrame:

    suspects_found_in_NIST = suspects_found_in_NIST.join(
        NIST.select(['NIST_ID','Synonyms_NIST']),on='NIST_ID',how='left')
    suspects_found_in_NIST = suspects_found_in_NIST.join(
        chromatogram.select(['Peak ID','RT (min)','energy_is_too_low', 'energy_is_too_high']),on='Peak ID',how='left')
    suspects_found_in_NIST = suspects_found_in_NIST.join(
        EPA.select(['DTXSID','Synonyms_EPA','CAS_EPA']),on='DTXSID',how='left')
    suspects_found_in_NIST = suspects_found_in_NIST.select(
    ['Peak ID',
    'RT (min)', 
    'Name_NIST', 
    'Haz_level', 
    'DotProd', 
    'Height', 
    'Precursor_mz_MSDIAL', 
    'Formula_NIST', 
    'NIST_ID',
    'collision_energy_NIST',
    'CAS_EPA', 
    'DTXSID', 
    'Synonyms_NIST',
    'Synonyms_EPA',
    'Precursor_type', 
    'MONOISOTOPIC MASS', 
    'inchikey', 
    'energy_is_too_low', 'energy_is_too_high',
    'isotopic_pattern_match'
    ]
    )

    return suspects_found_in_NIST

**************
7. NIST_search
**************
import polars as pl
import numpy as np
from pathlib import Path
import os
from spectral_similarity import identity_score_NIST_like_batch, entropy_score_batch
from time import time, sleep
from read_MSDIAL import get_chromatogram
import matplotlib.pyplot as plt
import shutil as sh 
from config import search_config

NIST_path = Path(r"")

def get_NIST(condig:search_config) -> pl.DataFrame:
    NIST = pl.scan_parquet(source=r"NIST_DB.parquet")
    NIST = NIST.select([
    'Name',
    'NIST_ID',
    'DB_ID',
    'DB_Name',
    'Precursor_type',
    'PrecursorMZ',
    'Ion_mode',
    'Instrument_type',
    'Formula',
    'Num_Peaks',
    'CAS',
    'InChIKey',
    'Synonyms',
    'raw_spectrum_intensity',
    'normalized_spectrum_mz',
    'MultiCharge',
    'Collision_energy_raw',
    # 'Collision_energy_NCE'
    # 'InChI',
    # 'CanonicalSMILES'
    ]).rename(
            {'InChIKey':'inchikey_NIST',
             'CAS':'CAS_NIST',
             'Synonyms':'Synonyms_NIST',
             'Name':'Name_NIST',
             'Formula':'Formula_NIST',
             'Precursor_type':'Precursor_type_NIST',
             'Collision_energy_raw':'collision_energy_NIST'
             }
        )
    

    if condig.polarity.lower()=="positive":
        mode="P"
    elif condig.polarity.lower()=="negative":
        mode="N"

    if mode is not None:
        NIST = NIST.filter(
            pl.col("Ion_mode").eq(mode)
    )
    else:
        print("supply correct operation mode to NIST_presearch_filtering to get better performance")
        NIST = NIST

    NIST = NIST.filter(
        pl.col('Instrument_type').eq('HCD') |
        pl.col('Instrument_type').eq('IT-FT/ion trap with FTMS'),
        pl.col('MultiCharge').not_()
    )
    NIST = NIST.collect()
    return NIST

def custom_search(
        query_df: pl.DataFrame | pl.LazyFrame, 
        NIST:pl.DataFrame | pl.LazyFrame,
        config:search_config,
    )->pl.DataFrame:
    match config.search_engine.lower():
        case 'entropy':
            function=entropy_score_batch
        case 'nir_cosine':
            function=identity_score_NIST_like_batch
        case 'cosine':
            function=identity_score_NIST_like_batch
        case _:
            print(config.search_engine)
            raise Exception("no valid search engine given")
        
    NIST_lf = NIST.select(
        [
            'NIST_ID',
            'PrecursorMZ',
            'raw_spectrum_intensity',
            'normalized_spectrum_mz',
        ]
    ).lazy()
    query_lf = query_df.lazy().select([
            'Peak ID',
            'Precursor_mz_MSDIAL',
            'msms_intensity',
            'msms_m/z'
            ])

    results = NIST_lf.join_where(
        query_lf,
        pl.col('PrecursorMZ').ge(pl.col('Precursor_mz_MSDIAL').mul(1-config.ms1_mass_tolerance)),
        pl.col('PrecursorMZ').le(pl.col('Precursor_mz_MSDIAL').mul(1+config.ms1_mass_tolerance)),
        suffix='_query'
    )
    results = results.collect(streaming=True)
    #using a UDF (User Defined Function) requires materializing in memory anyway, so it's better to collect
    results = results.with_columns(
        pl.struct(
            pl.col('raw_spectrum_intensity'),
            pl.col('normalized_spectrum_mz'),
            pl.col('msms_intensity'),
            pl.col('msms_m/z')
        ).map_batches(
            lambda spectra: function(
                spectra.struct.field('normalized_spectrum_mz').to_numpy(),
                spectra.struct.field('raw_spectrum_intensity').to_numpy(),
                spectra.struct.field('msms_m/z').to_numpy(),
                spectra.struct.field('msms_intensity').to_numpy(),
                config),
            return_dtype=pl.Float64,
            is_elementwise=True
        ).mul(1000.0).alias('DotProd')
    )
    results = results.select([
        'NIST_ID',
        'Peak ID',
        'DotProd'
        ])
    return results




################ functions beyond this point are for search using the NIST software, which is very slow. ##############

def NIST_search_external(
        query_df: pl.DataFrame, 
        MSDIAL_file_path: Path|str,
        NIST:pl.DataFrame| pl.LazyFrame,
        ignore_previous_results:bool=True,
        ) -> pl.DataFrame:
    
    NIST.select(['NIST_ID','DB_ID',"DB_Name"])

    searched = _searched_already(file_path=MSDIAL_file_path)
    if not searched['searched'] or ignore_previous_results:
        query_df = format_MSP(query_df)
        send_frame_to_NIST(query_df)
        _wait_for_NIST_ready()
        results_path = _save_search_results(data_path=MSDIAL_file_path)
        results = _read_NIST_eager(results_path=results_path)
    else:
        results = _read_NIST_eager(results_path=searched['path'])
    results = results.select(['Peak ID','DotProd','DB_Name','DB_ID'])
    results = results.join(NIST,on=['DB_Name','DB_ID'],how='left')
    results = results.drop(['DB_Name','DB_ID'])
    
    
    return results

# reads the current results file, saves it if needed (meaning no results were saved for the given file.)
# the path it takes is the path of the MSDIAL file it (presumably) reads the results of.
# waits for NIST to finish.
# also note- it first copies and saves the file, then reads from the sved file.
def read_NIST(file_path : str | Path) -> pl.DataFrame:
    _wait_for_NIST_ready()
    results = _read_NIST_eager(MSDIAL_file_path=file_path)
    return results

# reads the current results file, saves it if needed (if no results were saved already for the given file.)
# the path it takes is the path of the MSDIAL file it (presumably) reads the results of.
# does not wait for NIST to finish.
# also note- it first copies and saves the file, then reads from the sved file.
def _read_NIST_eager(
        results_path: str| Path) -> pl.DataFrame:
    if isinstance(results_path,str):
        results_path = Path(results_path)

    results = _read_SRCRESLT_type_file(results_path)
    # if results.item(0,0) is None:
    #     print("error in file: " + str(MSDIAL_file_path) + "no results found in nist after filtering based on library etc', so probavly wrong search settings in NIST")
    
    return results

# checks if a given MSDIAL export file was searched in NIST already
# and returns teh answer and the possible path of the result file.
def _searched_already(file_path : str | Path ) -> dict:
    '''checks if a given MSDIAL export file was searched in NIST already and returns the answer and the possible path of the result file.'''
    if isinstance(file_path,str):
        file_path = Path(file_path)
    possible_results_path = file_path.parent.joinpath(file_path.stem+'_NIST_results.txt')
    return {'searched':possible_results_path.exists(), 'path':possible_results_path}

def send_frame_to_NIST(chromatogram: pl.LazyFrame | pl.DataFrame):
    
    if isinstance(chromatogram,pl.DataFrame):
        if 'msp' not in chromatogram.columns:
            chromatogram = format_MSP(chromatogram)
        msp = chromatogram.select(['msp']).to_series()
    elif isinstance(chromatogram,pl.LazyFrame):
        if 'msp' not in chromatogram.collect_schema().names():
            chromatogram = format_MSP(chromatogram)
        msp = chromatogram.select(['msp']).collect().to_series()

    
    msp_to_print = msp.str.join(delimiter="\n\n")
    data_file_path = NIST_path.joinpath('data2nist.msp')
    msp_file = open(data_file_path,'w')
    msp_file.write(msp_to_print[0])
    msp_file.close()


    filespec = open(NIST_path.joinpath('filespec.fil'),'w')
    filespec.write(str(data_file_path) +' APPEND'+ '\n'+ '10 724')
    filespec.close()
    srcreslt = open(NIST_path.joinpath('SRCRESLT.txt'),'w')
    srcreslt.close()
    if os.path.isfile(NIST_path.joinpath('SRCREADY.txt')):
        os.remove(NIST_path.joinpath('SRCREADY.txt'))
    command = str(NIST_path) +r'\NISTMS$.EXE /INSTRUMENT /PAR=2' # 
    # print(command)
    os.system(command)

def _wait_for_NIST_ready():
    stp = 0
    while stp == 0:
        try:
            srcready = open(NIST_path.joinpath('SRCREADY.txt'),'r')
            srcready.close()
            sleep(2)  # waiting to make sure

            stp = 1
        except:
            sleep(10)
            continue
    return

def remove_HLMs():
    '''cleans all the *.HLM files nist search leaves behind'''
    for file in NIST_path.glob('*.HLM'):
        file.unlink()

def format_MSP(chromatogram: pl.LazyFrame | pl.DataFrame) -> pl.LazyFrame | pl.DataFrame:
    chromatogram = chromatogram.with_columns(
        msp_start=pl.concat_str(
            pl.lit("Name: "),
            pl.col('Peak ID'),
            pl.lit("\n"),
            pl.lit("PRECURSORMZ: "),
            pl.col('Precursor_mz_MSDIAL'),
            pl.lit("\n"),
            pl.lit("Num Peaks: "),
            pl.col('msms_m/z').list.len().cast(pl.String),
            pl.lit("\n"),
            # pl.col('MSMS spectrum') # TODO: remove this, use the proper lists
            ),
        msp_spectrum=pl.struct(
            pl.col('msms_m/z'),
            pl.col('msms_intensity')
            ).map_batches(
                function=lambda spectra: _convert_spectrum_to_text_batch(
                spectra.struct.field('msms_m/z').to_numpy(),
                spectra.struct.field('msms_intensity').to_numpy()
                ),
                return_dtype=pl.String,
                is_elementwise=True)
    ).with_columns(
        msp=pl.concat_str(
            pl.col('msp_start'),
            pl.col('msp_spectrum')
        )
    ).drop(['msp_start','msp_spectrum'])
    return chromatogram

def _convert_spectrum_to_text(mz:np.array,intensity:np.array) -> str:
    mz = mz.astype(np.str_)
    intensity = intensity.astype(np.str_)
    tabs = np.full_like(a=mz,fill_value='   ',dtype=np.str_)
    line_breaks = np.full_like(a=mz,fill_value='\n',dtype=np.str_)
    str_arr = np.stack((mz,tabs,intensity,line_breaks),axis=-1).flatten() # this interleaves them.
    string = ''.join(str_arr)
    return string
_convert_spectrum_to_text_batch = np.vectorize(_convert_spectrum_to_text)




def _read_SRCRESLT_type_file(file_path : str | Path ) -> pl.DataFrame:
    if isinstance(file_path,str):
        file_path = Path(file_path)
    srcrsults = open(file_path, mode='r',encoding='ANSI',errors="ignore")  # ignoring encoding errors
    search_results = srcrsults.read()
    search_results = search_results.split('\nUnknown: ')
    srcrsults.close()
    search_results[0] = search_results[0].strip("Unknown: ")
    NIST_results = pl.DataFrame(search_results,schema={'raw':pl.String})
    NIST_results = NIST_results.with_columns(
        pl.col('raw').str.extract(pattern=r"^(\d+)",group_index=1).str.to_integer().alias('Peak ID'))
    
    
    # NIST_results = NIST_results.select(
    #     pl.col('raw').str.split(by='\n').explode()
    # )
    NIST_results = NIST_results.with_columns(
        pl.col('raw').str.split(by='\n').alias('raw')
    )
    NIST_results = NIST_results.explode('raw')
    # NIST_results.write_csv(file="a",separator= "|")
    NIST_results = NIST_results.with_columns(
        # pl.col('raw').str.extract(pattern=r"^(\d+)",group_index=1).str.to_integer().alias('Peak ID'),
        pl.col('raw').str.extract(pattern=r"; MF:(\s+)(\d+);",group_index=2).cast(pl.Int64)
            .alias('Score'),
        pl.col('raw').str.extract(pattern=r"; RMF:(\s+)(\d+);",group_index=2).cast(pl.Int64)
            .alias('DotProd'), #yes, after testing this was concluded to be the DotProd, not the RevDot.
        pl.col('raw').str.extract(pattern=r"; RMF:(\s+)(\d+);",group_index=2).cast(pl.Int64)
            .alias('revdot'),
        pl.col('raw').str.extract(pattern=r"Id:(\s+)(\d+)\.",group_index=2).cast(pl.Int64)
            .alias('DB_ID'),
        pl.col('raw').str.extract(pattern=r"Lib: <<(.+)>>;",group_index=1)
            .alias('DB_Name'),
        pl.col('raw').str.extract(pattern=r"Hit \d{1}  : <<(.+?)\[",group_index=1)
            .alias('Name_of_NIST_hit'),
        pl.col('raw').str.extract(pattern=r"\s{2}\[(.+?)\]\d?[+-]",group_index=1)
            .alias('precursor_type'),
        pl.col('raw').str.extract(pattern=r"\s{2}\[(.+?)\](\d?[+-])",group_index=2)
            .alias('charge'),    
        pl.col('raw').str.extract(pattern=r"; CAS:(.+?);",group_index=1)
            .alias('CAS'),
        pl.col('raw').str.extract(pattern=r"(QTOF)|(IT\-FT)|(IT\s)|(HCD)|(QQQ)",group_index=0)
            .alias('instrument_type'),
        pl.col('raw').str.extract(pattern=r"P=(\d+(\.\d+)?)>>;",group_index=1).cast(pl.Float64)
            .alias('tag_mass'),
    )

    NIST_results = NIST_results.with_columns(
        precursor_type=pl.concat_str(
            pl.lit("["),
            pl.col('precursor_type'),
            pl.lit("]"),
            pl.col('charge')
        )
    )
    NIST_results = NIST_results.drop(['raw','charge']).sort(by='Peak ID',nulls_last=True)
    # NIST_results.write_csv(file="b",separator= "|")

    NIST_results = filter_NIST_results(NIST_results)
    return NIST_results

def read_NIST_default_location() -> pl.DataFrame:
    NIST_results = _read_SRCRESLT_type_file(file_path=NIST_path.joinpath('SRCRESLT.txt'))
    return NIST_results

# filters the results to be only of orbitrap, and from the right libraries
def filter_NIST_results(results:  pl.DataFrame) -> pl.DataFrame:
    filtered = results.filter(
        # pl.col('precursor_type').eq('[M+H]+'),
        pl.col('DB_Name').eq('hr_msms_nist#2') | pl.col('DB_Name').eq('hr_msms_nist') ,
        pl.col('instrument_type').eq('HCD') | pl.col('instrument_type').eq('IT-FT')
    )
    return filtered

#saves the results in a file with added "_NIST_results", and returns its path.
def _save_search_results(data_path: str | Path) -> Path:
    if isinstance(data_path,str):
        data_path = Path(data_path)
    results_path = data_path.parent.joinpath(data_path.stem+'_NIST_results.txt')
    sh.copyfile(src=NIST_path.joinpath('SRCRESLT.txt'),dst=results_path)
    return results_path

def plot_result_stats(NIST_results):
    NIST_results =  NIST_results.filter(pl.col('dot').is_not_null())
    hist = NIST_results.select('dot').to_series().to_numpy()
    vals = plt.hist(hist,bins=10,range=(500,1000))
    print(vals)
    plt.show()
    return

def print_result_stats(NIST_results):
    NIST_results =  NIST_results.filter(pl.col('dot').is_not_null())
    hist = NIST_results.select('dot').to_series().to_numpy()
    vals = np.histogram(hist,bins=10,range=(500,1000))
    print(vals)
    return


if __name__ == "__main__":
    start = time()

    print("time: "+str(time()-start))

**************
8. pyscreen_constants
**************
import numpy as np

mass_accuracy_1 = 0.0012 # in Da. This represents 3 ppm at 400 Da
mass_accuracy_2 = 0.0004 # in Da. This represents 1 ppm at 400 Da

#for isotopic pattern
NITROGEN_SEPARATION_RESOLUTION=1e5

adducts_pos = {
    "[M+H]+":1.007325,
    "[M+NH4]+": 18.034,
    "[M+Na]+":22.989
}

adducts_neg = {
    "[M-H]-":-1.007325
}


NIST_columns_to_read = [
    'Name',
    'NIST_ID',
    'DB_ID',
    'DB_Name',
    'Precursor_type',
    'PrecursorMZ',
    'Ion_mode',
    'Instrument_type',
    'Formula',
    'Num_Peaks',
    'CAS',
    'InChIKey',
    'Synonyms',
    'raw_spectrum_intensity',
    'normalized_spectrum_mz',
    'MultiCharge',
    'Collision_energy_raw'
    # 'InChI',
    # 'CanonicalSMILES'
]

MSDIAL_columns_to_read = [
    'Peak ID','Scan',
    'RT left(min)', 'RT (min)', 'RT right (min)',
    'Precursor m/z',
    'Height', 
    'Adduct','Isotope', 
    'MSMS spectrum',
    'MS1 isotopes'
]

MSDIAL_other_columns = [
    'Estimated noise', 'S/N',
    'Sharpness', 'Gaussian similarity', 'Ideal slope', 'Symmetry', 'MS1 isotopes' #'S/N', (second S/N has the same values as the first one.)
]


MSDIAL_columns_to_output= [
    'Peak ID',
    'RT (min)',
    'Precursor_mz_MSDIAL',
    'Height', 
    'Precursor_type_MSDIAL', 
    'msms_m/z', 'msms_intensity', 
    'isobars',
    'msms_m/z_cleaned', 'msms_intensity_cleaned',
    'spectral_entropy',
    'energy_is_too_low', 'energy_is_too_high',
    'ms1_isotopes_m/z', 'ms1_isotopes_intensity'
]

EPA_important_columns = [
    'DTXSID','Haz_level','MS_READY_SMILES',
    'PREFERRED NAME','CASRN','INCHIKEY',
    'IUPAC NAME','SMILES',
    'MOLECULAR FORMULA','MONOISOTOPIC MASS',
    'synonyms'
    ]
**************
9. read_MSDIAL
**************
import polars as pl
import numpy as np
from pathlib import Path
from time import time
from pyscreen_constants import MSDIAL_columns_to_read, MSDIAL_columns_to_output
from ms_entropy import calculate_spectral_entropy

def get_chromatogram(path: str | Path)-> pl.DataFrame :
    chromatogram = _get_chromatogram_basic(path=path)
    chromatogram = _annotate_isobars_and_clean_spectrum(chromatogram=chromatogram)
    chromatogram = _add_energy_annotation(chromatogram=chromatogram)
    chromatogram = _add_entropy(chromatogram=chromatogram)
    chromatogram = chromatogram.select(MSDIAL_columns_to_output)
    if not isinstance(chromatogram,pl.DataFrame):
        raise Exception("failed getting chromatogram from the file: " + str(path))
    
    return chromatogram

def _get_chromatogram_basic(path: str | Path)-> pl.LazyFrame :
    chromatogram=pl.read_csv(source=path,has_header=True,skip_rows=0,separator="	", null_values='null')
    chromatogram = chromatogram.select(MSDIAL_columns_to_read)
    chromatogram=_convert_MSMS_to_list(chromatogram).drop('MSMS spectrum')
    chromatogram = _convert_MS1_to_list(chromatogram).drop('MS1 isotopes')
    chromatogram=chromatogram.with_columns(
        pl.col('RT right (min)').sub(pl.col('RT left(min)')).alias('peak_width_min'),
        pl.col('Precursor m/z').round(0).cast(pl.Int64).alias('nominal_mass'),
        pl.col('RT (min)').mul(60).round(0).cast(pl.Int64).alias('RT_(sec)'),
        pl.col('Precursor m/z').round(4).alias('Precursor m/z'),
        
    ).rename(
        {
        'Precursor m/z':'Precursor_mz_MSDIAL',
        'Adduct':'Precursor_type_MSDIAL', 
        }
    )
    
    return chromatogram

def _add_energy_annotation(chromatogram:pl.DataFrame) -> pl.DataFrame:
    chromatogram_with_msms = chromatogram.filter(pl.col('msms_m/z').is_not_null())
    chromatogram_with_msms = chromatogram_with_msms.with_columns( # get the index of the molecular ion, if it even exists
        molecular_ion_index=(pl.col('msms_m/z')-pl.col('Precursor_mz_MSDIAL')).list.eval(pl.element().abs()).list.arg_min()
    ) #this will return an index even if there is no molecular ion.
    chromatogram_with_msms = chromatogram_with_msms.with_columns(
        molecular_ion_intensity=pl.when(
            (pl.col('msms_m/z').list.get(pl.col('molecular_ion_index')) - pl.col('Precursor_mz_MSDIAL'))<0.003 # 3 mDa as the tolerance
        ).then(pl.col('msms_intensity').list.get(pl.col('molecular_ion_index'))).otherwise(pl.lit(0)),
        second_highest_intensity=pl.col('msms_intensity').list.sort(descending=True,nulls_last=True).list.get(1) # so the second highest intesity
    )
    chromatogram_with_msms = chromatogram_with_msms.with_columns(
        pl.col('molecular_ion_intensity').le(0.1).alias('energy_is_too_high'),
        (pl.col('molecular_ion_intensity').eq(1)&pl.col('second_highest_intensity').le(0.2)).alias('energy_is_too_low')
    ).select(['Peak ID','energy_is_too_high','energy_is_too_low'])
    return chromatogram.join(other=chromatogram_with_msms,on='Peak ID',how='left')

def _add_entropy(chromatogram:pl.DataFrame)-> pl.DataFrame:
    chromatogram = chromatogram.with_columns(
        pl.struct(
            pl.col('msms_m/z'),
            pl.col('msms_intensity')
        ).map_batches(
            lambda spectra: calculate_spectral_entropy_wrapper_batch(
                spectra.struct.field('msms_m/z').to_numpy(),
                spectra.struct.field('msms_intensity').to_numpy()),
            return_dtype=pl.Float32,
            is_elementwise=True
        ).alias('spectral_entropy')
    )
    return chromatogram

def calculate_spectral_entropy_wrapper(mz,intesity):
    spectrum = np.column_stack((mz,intesity))
    spectrum = np.array(spectrum,dtype=np.float32)
    return calculate_spectral_entropy(spectrum)
calculate_spectral_entropy_wrapper_batch=np.vectorize(calculate_spectral_entropy_wrapper)

def _convert_MSMS_to_list(chromatogram: pl.LazyFrame | pl.DataFrame) -> pl.LazyFrame | pl.DataFrame:

    chromatogram = chromatogram.with_columns(
        pl.col('MSMS spectrum').str.extract_all(
            pattern=r'(\d+\.\d+)'
        ).list.eval(pl.element().str.to_decimal().cast(pl.Float64)).alias('msms_m/z'),
        pl.col('MSMS spectrum').str.extract_all(
            pattern=r'(\d+)\s|(\d+$)'
        ).list.eval(pl.element().str.extract( pattern=r'(\d+)').str.to_decimal().cast(pl.Float64).round(4)).alias('msms_intensity')
        #).alias('msms_intensity')
    )
    chromatogram = chromatogram.with_columns(
        pl.col('msms_intensity').truediv(pl.col('msms_intensity').list.max())
    )

    return chromatogram

def _convert_MS1_to_list(chromatogram: pl.LazyFrame | pl.DataFrame) -> pl.LazyFrame | pl.DataFrame:

    chromatogram = chromatogram.with_columns(
        pl.col('MS1 isotopes').str.extract_all(
            pattern=r'(\d+\.\d+)'
        ).list.eval(pl.element().str.to_decimal().cast(pl.Float64)).alias('ms1_isotopes_m/z'),
        pl.col('MS1 isotopes').str.extract_all(
            pattern=r'(\d+)\s|(\d+$)'
        ).list.eval(pl.element().str.extract( pattern=r'(\d+)').str.to_decimal().cast(pl.Float64).round(4)).alias('ms1_isotopes_intensity')
    )
    # removed because we need to know the actual intensity of each, not only the relative.
    # chromatogram = chromatogram.with_columns(
    #     pl.col('ms1_isotopes_intensity').truediv(
    #         pl.col('ms1_isotopes_intensity').list.get(
    #             pl.col('ms1_isotopes_m/z').sub(pl.col('Precursor m/z')).list.eval(pl.element().abs()).list.arg_min()
    #         )
    #     )
    # )
    return chromatogram


def _annotate_isobars_and_clean_spectrum(chromatogram: pl.LazyFrame | pl.DataFrame) -> pl.DataFrame:
    chromatogram = chromatogram.lazy()
    chromatogram_with_msms = chromatogram.filter(pl.col('msms_intensity').is_not_null()) #why? cause otherwise we don't know how to subtract spectrum

    
    isobars = chromatogram_with_msms.join_where(
        chromatogram_with_msms,
        # pl.col('Precursor_mz_MSDIAL').round(decimals=0).eq(pl.col('Precursor_mz_MSDIAL_isobar').round(decimals=0)),
        # pl.col('Precursor_mz_MSDIAL').round(decimals=0).cast(pl.UInt16).eq(pl.col('Precursor_mz_MSDIAL_isobar').round(decimals=0).cast(pl.UInt16)),
        pl.col('nominal_mass').eq(pl.col('nominal_mass_isobar')),
        pl.col('RT_(sec)').sub(pl.col('RT_(sec)_isobar')).abs().le(pl.lit(6,dtype=pl.Int64)), #less than 6 seconds of difference
        pl.col('Height').truediv(pl.col('Height_isobar')).le(pl.lit(3,dtype=pl.Int64)), #the contaminant is at least a third as high
        pl.col('Peak ID').ne(pl.col('Peak ID_isobar')) # to prevent compunds from being the isobars of themselves
        ,suffix='_isobar'
        )

    isobars = isobars.group_by('Peak ID').all()
    isobars = isobars.with_columns(pl.col('Peak ID_isobar').alias('isobars'))
    isobars = isobars.select(['Peak ID','isobars'])

    chromatogram = chromatogram.join(isobars,on='Peak ID',how='left')
    chromatogram = chromatogram.collect()
    
    only_with_isobars = chromatogram.filter(pl.col('isobars').is_not_null())

    # ugly workaround. didn't find a better way.
    only_with_isobars_rows = only_with_isobars.select(['Peak ID','msms_m/z','msms_intensity','RT (min)','isobars','Height']).rows_by_key(key=['Peak ID'],named=True,unique=True)
    chromatogram_rows = chromatogram.rows_by_key(key=['Peak ID'],named=True,unique=True)

    for compound in only_with_isobars_rows:
        isobars = only_with_isobars_rows[compound]['isobars']
        for isobar in isobars:
            only_with_isobars_rows[compound]['msms_m/z'], only_with_isobars_rows[compound]['msms_intensity'] = _subtract_isobar_spectra( # subtracts the second from the first
                only_with_isobars_rows[compound]['msms_m/z'], 
                only_with_isobars_rows[compound]['msms_intensity'],
                only_with_isobars_rows[compound]['RT (min)'], 
                only_with_isobars_rows[compound]['Height'],
                chromatogram_rows[isobar]['msms_m/z'],
                chromatogram_rows[isobar]['msms_intensity'],
                chromatogram_rows[isobar]['RT (min)'],
                chromatogram_rows[isobar]['Height']
                )


    # this block just rearanges the data to a dict of {"Peak ID" : [the IDs], "data1":[the data] etc}
    cleaned_rows = []
    for ID, labels in only_with_isobars_rows.items():
        new_row = {"Peak ID": ID}
        new_row.update(labels)
        cleaned_rows.append(new_row)
    result_dict = {}
    for row in cleaned_rows:
        for key, value in row.items():
            if key not in result_dict:
                result_dict[key] = []
            result_dict[key].append(value)

    chromatogram3 = pl.DataFrame(result_dict) # simple as that
    chromatogram3 = chromatogram3.select(['Peak ID','msms_m/z','msms_intensity'])

    chromatogram=chromatogram.join(chromatogram3, on="Peak ID",how="left", suffix="_cleaned")
    chromatogram = chromatogram.with_columns( #converts empty lists to null
        pl.when(
        pl.col('msms_m/z_cleaned').list.len().gt(0)
        ).then(pl.col('msms_m/z_cleaned')),
        pl.when(
        pl.col('msms_intensity_cleaned').list.len().gt(0)
        ).then(pl.col('msms_intensity_cleaned'))) 
    

    return chromatogram

def _subtract_isobar_spectra(
        compound_msms_mz,compound_msms_intensity,
        compound_RT, compound_height,
        isobar_msms_mz,isobar_msms_intensity,
        isobar_RT,isobar_height):

    rt_diff= compound_RT - isobar_RT
    coeff = np.exp(-np.power(rt_diff,2)*10) *(isobar_height/compound_height)
    coeff = np.full_like(isobar_msms_intensity,fill_value=coeff)
    adj_isobar_msms_intensity = np.multiply(coeff,isobar_msms_intensity)

    compound_spectra_dict = dict(zip(compound_msms_mz,compound_msms_intensity))
    isobar_spectra_dict = dict(zip(isobar_msms_mz,adj_isobar_msms_intensity))
    compound_spectra_dict = {mz: (compound_spectra_dict[mz] - isobar_spectra_dict.get(mz,0)) for mz in compound_spectra_dict.keys()}
    
    compound_spectra_dict = {mz: intensity for mz, intensity in compound_spectra_dict.items() if intensity > 0 }

    compound_msms_mz = np.array(list(compound_spectra_dict.keys()),dtype=np.float64)
    compound_msms_intensity = np.array(list(compound_spectra_dict.values()),dtype=np.float64)

    return compound_msms_mz,compound_msms_intensity
    



if __name__ == '__main__':
    start = time()
    pl.Config(
    tbl_rows=20,
    tbl_cols=15)
    path = Path(r'')
    
    chromatogram = get_chromatogram(path=path)

    if isinstance(chromatogram,pl.LazyFrame):
        print(chromatogram.collect_schema())
        print(chromatogram.collect())
    elif isinstance(chromatogram,pl.DataFrame):
        print(chromatogram.schema)
        print(chromatogram)
    else:
        print("wrong output! this must be either a polars lazyframe or dataframe")
        print(type(chromatogram))


    print(time()-start)

**************
10. spectral_similarity
**************
import numpy as np
import polars as pl
from numba import njit
from time import time
from ms_entropy import calculate_entropy_similarity
from config import search_config

# yeah this is a bit of a mess, but splitting to multiple functions didn't behave nicely with numba
@njit
def identity_score_NIST_like(
        spec1_mz:np.ndarray, spec1_intensity:np.ndarray,
        spec2_mz:np.ndarray, spec2_intensity:np.ndarray,
        config:search_config
        )-> np.float64:
    """ """
    ms2_mass_tolerance_search = config.ms2_mass_tolerance

    shift=np.float64(0.0)
    mz_power=np.float64(2)
    intensity_power=np.float64(0.5)
    #### find_peak_matches
    lowest_idx = 0
    matches = []
    for i in range(spec1_mz.shape[0]):
        mz1 = spec1_mz[i]
        mz_min = mz1 - ms2_mass_tolerance_search*mz1
        mz_max = mz1 + ms2_mass_tolerance_search*mz1
        for j in range(lowest_idx, spec2_mz.shape[0]):
            mz2 = spec2_mz[j] + shift
            if mz2 > mz_max:
                break
            if mz2 < mz_min:
                lowest_idx = j + 1
            else:

                matches.append((i,j))
    #### end of "find_peak_matches"
    idx1 = np.array([x[0] for x in matches])
    idx2 = np.array([x[1] for x in matches])

    if len(idx1) == 0:
        return 0.0
    score = 0.0
    for i in range(idx1.shape[0]):
        score += (
            np.power(spec1_intensity[idx1[i]]*spec2_intensity[idx2[i]],intensity_power) * 
            np.power((spec1_mz[idx1[i]]*spec2_mz[idx2[i]]),mz_power)
            )

    

    # spec1_norm = cosine_score_self(mz1, spec_intensity1, mz_tol_ppm, mz_power, intensity_power)
    
    # this is "find_peak_matches_self" for spec1
    lowest_idx = 0
    matches = []
    for i in range(spec1_mz.shape[0]):
        mz1 = spec1_mz[i]
        mz_min = mz1 - ms2_mass_tolerance_search*mz1
        mz_max = mz1 + ms2_mass_tolerance_search*mz1
        for j in range(lowest_idx, spec1_mz.shape[0]):
            mz2 = spec1_mz[j]
            if mz2 > mz_max:
                break
            if mz2 < mz_min:
                lowest_idx = j + 1
            else:
                matches.append((i,j))
    idx1 = np.array([x[0] for x in matches])
    idx2 = np.array([x[1] for x in matches])
    spec1_norm = np.float64(0.0)
    for i in range(idx1.shape[0]):
        spec1_norm += (
            np.power(spec1_intensity[idx1[i]]*spec1_intensity[idx2[i]],intensity_power) * 
            np.power((spec1_mz[idx1[i]]*spec1_mz[idx2[i]]),mz_power))

    # this is "find_peak_matches_self" for spec2
    lowest_idx = 0
    matches = []
    for i in range(spec2_mz.shape[0]):
        mz1 = spec2_mz[i]
        mz_min = mz1 - ms2_mass_tolerance_search*mz1
        mz_max = mz1 + ms2_mass_tolerance_search*mz1
        for j in range(lowest_idx, spec2_mz.shape[0]):
            mz2 = spec2_mz[j]
            if mz2 > mz_max:
                break
            if mz2 < mz_min:
                lowest_idx = j + 1
            else:
                matches.append((i,j))
    idx1 = np.array([x[0] for x in matches])
    idx2 = np.array([x[1] for x in matches])
    spec2_norm = np.float64(0.0)
    for i in range(idx1.shape[0]):
        spec2_norm += (
            np.power(spec2_intensity[idx1[i]]*spec2_intensity[idx2[i]],intensity_power) * 
            np.power((spec2_mz[idx1[i]]*spec2_mz[idx2[i]]),mz_power))


    score = score / np.sqrt(spec1_norm*spec2_norm)

    return score
identity_score_NIST_like_batch = np.vectorize(identity_score_NIST_like)



def entropy_score(
        spec1_mz:np.ndarray, spec1_intensity:np.ndarray,
        spec2_mz:np.ndarray, spec2_intensity:np.ndarray,
        config:search_config) -> np.float64:
    if any(x is None for x in [spec1_mz,spec2_mz,spec1_intensity,spec2_intensity]):
        return -1
    spec1 = np.column_stack((spec1_mz,spec1_intensity))
    spec1 = np.array(spec1,dtype=np.float32)
    spec2 = np.column_stack((spec2_mz,spec2_intensity))
    spec2 = np.array(spec2,dtype=np.float32)
    score = calculate_entropy_similarity(
        spec1,spec2,
        ms2_tolerance_in_ppm=config.ms2_mass_tolerance*10e6,
        clean_spectra=True,
        noise_threshold=config.noise_threshold)
    score = np.float64(score)
    return score
entropy_score_batch=np.vectorize(entropy_score)

def NIST_filtering_mock(NIST:pl.DataFrame) -> pl.DataFrame:
    NIST_filtered = NIST.filter(
        pl.col('Instrument_type').eq('HCD'),
        pl.col('MultiCharge').not_()
    ).select(
        ['Name','NIST_ID','PrecursorMZ','Precursor_type',
         'raw_spectrum_intensity','normalized_spectrum_mz']
    )
    return NIST_filtered

def NIST_search_mock(
        query_df: pl.DataFrame, 
        NIST:pl.DataFrame,
        mz_tol_ppm=5) -> pl.DataFrame:
    results = NIST.join_where(
        query_df,
        pl.col('PrecursorMZ').ge(pl.col('PrecursorMZ_query').mul(1-mz_tol_ppm/1e6)),
        pl.col('PrecursorMZ').le(pl.col('PrecursorMZ_query').mul(1+mz_tol_ppm/1e6)),
        suffix='_query'
    )

    results = results.with_columns(
        pl.struct(
            pl.col('raw_spectrum_intensity'),
            pl.col('raw_spectrum_mz'),
            pl.col('raw_spectrum_intensity_query'),
            pl.col('raw_spectrum_mz_query')
        ).map_batches(
            lambda spectra: identity_score_NIST_like_batch(
                spectra.struct.field('raw_spectrum_mz').to_numpy(),
                spectra.struct.field('raw_spectrum_intensity').to_numpy(),
                spectra.struct.field('raw_spectrum_mz_query').to_numpy(),
                spectra.struct.field('raw_spectrum_intensity_query').to_numpy())
        ).alias('cosine_score')
    )

    results = results.select(['NIST_ID','NIST_ID_query','cosine_score']).sort('cosine_score',descending=True)
    return results


if __name__ == "__main__":
    pl.set_random_seed(42)
    start = time()
    ms1_mass_tolerance = 5
    # NIST = scan_DB(DB_path=r"NIST_DB.parquet").select(
    #     ['Name','NIST_ID','PrecursorMZ',
    #      'raw_spectrum_intensity','raw_spectrum_mz',
    #      'Instrument_type','MultiCharge']
    # ).collect()
    # NIST = NIST_filtering_mock(NIST)
    # print(time()-start)
    # after_read = time()



    # query_df = NIST.sample(20000,seed=42)
    # time1 = time()
    # results = NIST_search_mock(query_df, NIST,mz_tol_ppm)
    # results=results.filter(
    #     pl.col('NIST_ID').ne(pl.col('NIST_ID_query'))
    # )
    # print(results)

    # best_matches = results.group_by('NIST_ID_query').agg(
    #     pl.col('NIST_ID').first().alias('NIST_ID'),
    #     pl.col('cosine_score').first().alias('cosine_score')).sort('cosine_score',descending=True)
    # print(best_matches)
    # print(time()-after_read)

    spec1_mz= np.array([400,400*(1+7*1e-6)])
    spec_intensity1= np.array([999,999])
    spec2_mz= np.array([400]) 
    spec_intensity2= np.array([999])
    print(identity_score_NIST_like(
        spec1_mz=spec1_mz,
        spec1_intensity=spec_intensity1,
        spec2_mz=spec2_mz,
        spec2_intensity=spec_intensity2))

**************
